# Verifying Ollama Usage in Report Generation

## Quick Check

When generating a report in CNS Master Agent, check the Firebase Functions logs for these indicators:

## âœ… Ollama is Being Used (Look for these logs):

```
[DocumentAnalysisService] ğŸ”§ Initializing DocumentAnalysisService...
[DocumentAnalysisService] âœ… Ollama service initialized successfully
[DocumentAnalysisService] ğŸ¯ Ollama will be used for report analysis (preferred over Gemini)
[DocumentAnalysisService] ğŸ¤– Attempting to use Ollama for analysis...
[DocumentAnalysisService] âœ… Ollama is available - using Ollama for analysis
[OllamaAnalysisService] âœ… Selected model: gemma3:12b (or phi4-mini)
[OllamaAnalysisService] ğŸ¤– Model: gemma3:12b
[OllamaAnalysisService] ğŸ’° Cost: $0 (local processing, private)
[DocumentAnalysisService] ğŸ‰ Report analysis completed using OLLAMA (local, private, $0 cost)
```

## âŒ Gemini is Being Used (Look for these logs):

```
[DocumentAnalysisService] ğŸ”µ Using Gemini for analysis (cloud service)
[DocumentAnalysisService] âš ï¸ NOTE: Data will be sent to Google cloud for processing
[DocumentAnalysisService] ğŸ”µ Report analysis completed using GEMINI (cloud, ~$0.01-0.05 cost)
```

## Environment Variables Required

To ensure Ollama is used, set these in your Firebase Functions environment:

```bash
REPORT_USE_OLLAMA=true
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL_FAST=phi4-mini
OLLAMA_MODEL_QUALITY=gemma3:12b
```

## Model Selection Logic

The system automatically selects the best model:

- **Financial reports** â†’ `gemma3:12b` (quality, accuracy critical)
- **Detailed reports** â†’ `gemma3:12b` (comprehensive analysis)
- **Executive reports** â†’ `phi4-mini` (speed priority)
- **Large context (>5K tokens)** â†’ `gemma3:12b` (128K context window)

## Troubleshooting

### If Gemini is being used instead of Ollama:

1. **Check environment variables:**
   ```bash
   firebase functions:config:get
   # Should show REPORT_USE_OLLAMA=true
   ```

2. **Check Ollama is running:**
   ```bash
   curl http://localhost:11434/api/tags
   # Should return list of models
   ```

3. **Check logs for errors:**
   ```
   [DocumentAnalysisService] âš ï¸ Ollama initialization failed
   [OllamaAnalysisService] âŒ Ollama is not available
   ```

4. **Verify models are installed:**
   ```bash
   ollama list
   # Should show phi4-mini and/or gemma3:12b
   ```

## Expected Log Flow (Ollama Success)

```
1. [DocumentAnalysisService] ğŸ”§ Initializing...
2. [DocumentAnalysisService] âœ… Ollama service initialized
3. [ReportGeneratorService] ğŸ“Š Starting report generation...
4. [DocumentAnalysisService] ğŸ¤– Attempting to use Ollama...
5. [OllamaAnalysisService] ğŸ” Checking Ollama availability...
6. [OllamaAnalysisService] âœ… Ollama is available
7. [OllamaAnalysisService] ğŸ¯ Selecting best model...
8. [OllamaAnalysisService] âœ… Selected model: gemma3:12b
9. [OllamaAnalysisService] ğŸ¤– Model: gemma3:12b
10. [OllamaAnalysisService] âœ… Analysis generated successfully
11. [DocumentAnalysisService] ğŸ‰ Report analysis completed using OLLAMA
```

## Key Indicators

- âœ… **"using Ollama for analysis"** = Ollama is being used
- âœ… **"Model: gemma3:12b"** or **"Model: phi4-mini"** = Specific model selected
- âœ… **"Cost: $0"** = Local processing (Ollama)
- âŒ **"Using Gemini"** = Cloud service (Gemini)
- âŒ **"~$0.01-0.05 cost"** = Cloud service (Gemini)
